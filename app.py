import os
import json
from typing import List, Tuple

import pandas as pd
import streamlit as st

# Google Gemini
import google.generativeai as genai


# -----------------------
# é¡µé¢ä¸åŸºç¡€è®¾ç½®
# -----------------------
st.set_page_config(page_title="Amazon è¯„ä»·åˆ†æ - Gemini", page_icon="ğŸ›’", layout="wide")

st.title("ğŸ›’ Amazon äº§å“è¯„ä»·åˆ†æï¼ˆGeminiï¼‰")
st.caption("ä¸Šä¼  CSV/Excelï¼ˆæ¯è¡Œä¸€æ¡è¯„ä»·ï¼‰ï¼Œé€æ¡æç‚¼ä¼˜ç¼ºç‚¹ï¼Œå¹¶æ”¯æŒä¸€é”®ç”Ÿæˆæ±‡æ€»ï¼ˆæµå¼è¾“å‡ºï¼‰")


# -----------------------
# ä¾§è¾¹æ è®¾ç½®
# -----------------------
with st.sidebar:
    st.header("è®¾ç½®")
    default_key = os.getenv("GOOGLE_API_KEY", "")
    api_key = st.text_input("Google Gemini API Key", value=default_key, type="password", help="ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ GOOGLE_API_KEY")
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gemini-1.5-flash", "gemini-1.5-pro"], index=0, help="Flash æ›´å¿«æ›´çœï¼ŒPro æ›´å¼ºä½†æ›´æ…¢")
    max_items = st.slider("æ¯æ¡è¯„ä»·æœ€å¤šæç‚¼æ¡æ•°ï¼ˆä¼˜/ç¼ºç‚¹å„ï¼‰", 1, 8, 5, 1)
    st.markdown("---")
    st.info("æç¤ºï¼šè¯·ç¡®ä¿ç½‘ç»œå¯è®¿é—® Gemini æœåŠ¡ã€‚å¦‚éœ€å®‰å…¨ä¿ç®¡å¯†é’¥ï¼Œå»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡ GOOGLE_API_KEYã€‚")


# -----------------------
# ä¼šè¯çŠ¶æ€
# -----------------------
if "analysis_results" not in st.session_state:
    # List[dict]: {index, review, pros[List[str]], cons[List[str]]}
    st.session_state.analysis_results = []
if "analysis_params" not in st.session_state:
    st.session_state.analysis_params = {}
if "summary_done" not in st.session_state:
    st.session_state.summary_done = False


# -----------------------
# å·¥å…·å‡½æ•°
# -----------------------
def init_gemini(_api_key: str):
    if not _api_key:
        raise ValueError("ç¼ºå°‘ API Key")
    genai.configure(api_key=_api_key)


def read_dataframe(uploaded_file) -> pd.DataFrame:
    if uploaded_file is None:
        return pd.DataFrame()
    name = uploaded_file.name.lower()
    if name.endswith(".csv"):
        # ç®€å•ç¼–ç å…œåº•
        try:
            df = pd.read_csv(uploaded_file)
        except UnicodeDecodeError:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding="latin1")
        return df
    if name.endswith(".xlsx") or name.endswith(".xls"):
        return pd.read_excel(uploaded_file)
    st.error("ä»…æ”¯æŒ CSVã€XLSX æˆ– XLS æ–‡ä»¶")
    return pd.DataFrame()


def guess_text_columns(df: pd.DataFrame) -> List[str]:
    if df.empty:
        return []
    # ä¼˜å…ˆ object ç±»å‹ï¼Œå…œåº•å…¨éƒ¨åˆ—
    text_cols = list(df.select_dtypes(include=["object"]).columns)
    if not text_cols:
        text_cols = list(df.columns)
    return text_cols


def unique_preserve(seq: List[str], limit: int) -> List[str]:
    seen = set()
    out = []
    for x in seq:
        x = (x or "").strip()
        if not x:
            continue
        if x not in seen:
            seen.add(x)
            out.append(x)
        if len(out) >= limit:
            break
    return out


def build_extract_prompt(review: str, max_items_each: int) -> str:
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸­ç«‹çš„äº§å“è¯„ä»·åˆ†æåŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹ç”¨æˆ·è¯„ä»·ä¸­æç‚¼è¯¥äº§å“çš„ä¼˜ç‚¹ä¸ç¼ºç‚¹ï¼Œè¦æ±‚ï¼š
- åªå…³æ³¨äº§å“æœ¬èº«çš„ç‰¹æ€§ä¸ä½“éªŒï¼›é™¤éç›´æ¥å½±å“äº§å“ä½“éªŒï¼Œå¦åˆ™å¿½ç•¥ç‰©æµ/å®¢æœ/åŒ…è£…/ä»·æ ¼æ³¢åŠ¨ç­‰ä¸äº§å“æ— å…³çš„ä¿¡æ¯ã€‚
- æ¯ç±»æœ€å¤šæç‚¼ {max_items_each} æ¡ï¼›çŸ­è¯­å½¢å¼ã€å»é‡ã€ç”¨ç®€ä½“ä¸­æ–‡ã€‚
- è‹¥æ— æ˜ç¡®ä¿¡æ¯ï¼Œå¯è¿”å›ç©ºåˆ—è¡¨ã€‚

åªè¾“å‡º JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "pros": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2", "..."],
  "cons": ["ç¼ºç‚¹1", "ç¼ºç‚¹2", "..."]
}}

ç”¨æˆ·è¯„ä»·ï¼š
\"\"\"{review}\"\"\"
""".strip()


def analyze_one_review(review: str, model_name: str, max_items_each: int) -> Tuple[List[str], List[str]]:
    # é‡‡ç”¨ JSON å“åº”ï¼Œé™ä½è§£æå¤±è´¥æ¦‚ç‡
    model = genai.GenerativeModel(
        model_name=model_name,
        generation_config={"response_mime_type": "application/json"},
    )
    prompt = build_extract_prompt(review, max_items_each)
    try:
        resp = model.generate_content(prompt)
        text = (resp.text or "").strip()
        # å»æ‰å¯èƒ½çš„ä»£ç å›´æ 
        if text.startswith("```"):
            text = text.strip("`")
            # å¤„ç†ä¾‹å¦‚ ```json ... ```
            parts = text.split("\n", 1)
            if len(parts) == 2:
                text = parts[1]
            text = text.split("```")[0].strip()

        data = json.loads(text)
        pros = data.get("pros", [])
        cons = data.get("cons", [])
        # è§„èŒƒåŒ–
        pros = [str(x).strip() for x in pros if str(x).strip()]
        cons = [str(x).strip() for x in cons if str(x).strip()]
        pros = unique_preserve(pros, max_items_each)
        cons = unique_preserve(cons, max_items_each)
        return pros, cons
    except Exception as e:
        # è¿”å›ç©ºï¼Œé¿å…ä¸­æ–­æ•´ä½“æµç¨‹
        return [], [f"è§£æå¤±è´¥æˆ–è°ƒç”¨é”™è¯¯ï¼š{e}"]


def build_summary_prompt(all_pros: List[str], all_cons: List[str]) -> str:
    pros_block = "\n".join([f"- {p}" for p in all_pros[:300]])  # æ§åˆ¶é•¿åº¦
    cons_block = "\n".join([f"- {c}" for c in all_cons[:300]])
    return f"""
ä½ å°†åŸºäºä»¥ä¸‹æ¥è‡ªç”¨æˆ·è¯„ä»·æç‚¼çš„â€œä¼˜ç‚¹/ç¼ºç‚¹â€è¿›è¡Œäº§å“çº§æ€»ç»“ï¼Œè¯·ï¼š
- å»é‡ä¸å½’ç±»ï¼ˆåˆå¹¶è¡¨è¾¾ç›¸ä¼¼çš„ç‚¹ï¼‰ï¼ŒæŒ‰é‡è¦æ€§æ’åºï¼›
- è¾“å‡ºç»“æ„ï¼š
  1) å…³é”®ä¼˜ç‚¹ï¼ˆåˆ†ç»„ï¼Œç»„å+1-2å¥è¯´æ˜ï¼‰
  2) å…³é”®ä¸è¶³ï¼ˆåˆ†ç»„ï¼Œç»„å+1-2å¥è¯´æ˜ï¼‰
  3) ç»“è®ºä¸å»ºè®®ï¼ˆ3-5æ¡ï¼Œé¢å‘äº§å“ä¼˜åŒ–ä¸è¥é”€ç­–ç•¥ï¼Œå¯æ‰§è¡Œï¼‰
- ç”¨ç®€ä½“ä¸­æ–‡ï¼Œé¿å…é‡å¤å’Œç©ºè¯ã€‚

ä¼˜ç‚¹ï¼ˆæ ·æœ¬ï¼‰ï¼š
{pros_block}

ç¼ºç‚¹ï¼ˆæ ·æœ¬ï¼‰ï¼š
{cons_block}
""".strip()


def stream_summary(all_pros: List[str], all_cons: List[str], model_name: str):
    # æµå¼ç”Ÿæˆä¸æŒ‡å®š JSON
    model = genai.GenerativeModel(model_name=model_name)
    prompt = build_summary_prompt(all_pros, all_cons)
    response = model.generate_content(prompt, stream=True)
    for chunk in response:
        try:
            if hasattr(chunk, "text") and chunk.text:
                yield chunk.text
        except Exception:
            continue


# -----------------------
# ä¸Šä¼ ä¸åˆ—é€‰æ‹©
# -----------------------
uploaded = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ï¼ˆCSV/Excelï¼Œæ¯è¡Œä¸€æ¡è¯„ä»·ï¼‰", type=["csv", "xlsx", "xls"])

df = read_dataframe(uploaded)
if not df.empty:
    st.subheader("æ•°æ®é¢„è§ˆ")
    st.dataframe(df.head(20), use_container_width=True)

col_options = guess_text_columns(df) if not df.empty else []
selected_col = None
if col_options:
    selected_col = st.selectbox("é€‰æ‹©è¯„ä»·æ–‡æœ¬æ‰€åœ¨åˆ—", col_options, index=0)

# -----------------------
# åˆ†ææŒ‰é’®ä¸è¿‡ç¨‹
# -----------------------
col_btn1, col_btn2 = st.columns([1, 1])

start_clicked = col_btn1.button("å¼€å§‹åˆ†æ", type="primary", disabled=(df.empty or selected_col is None))
summary_clicked = col_btn2.button("ç”Ÿæˆæ±‡æ€»ï¼ˆæµå¼è¾“å‡ºï¼‰", disabled=(len(st.session_state.analysis_results) == 0))

results_placeholder = st.empty()
progress_placeholder = st.empty()
status_placeholder = st.empty()
summary_placeholder = st.empty()

if start_clicked:
    st.session_state.summary_done = False

    if not api_key:
        st.error("è¯·åœ¨å·¦ä¾§è¾“å…¥ Google Gemini API Keyï¼ˆæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_API_KEYï¼‰")
    else:
        try:
            init_gemini(api_key)
        except Exception as e:
            st.error(f"åˆå§‹åŒ– Gemini å¤±è´¥ï¼š{e}")
        else:
            # æ¸…ç©ºæ—§ç»“æœ
            st.session_state.analysis_results = []

            # æ¸…æ´—æ–‡æœ¬
            reviews_series = df[selected_col].fillna("").astype(str).map(lambda x: x.strip())
            reviews = [r for r in reviews_series.tolist() if r]

            total = len(reviews)
            if total == 0:
                st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä»·æ–‡æœ¬ã€‚")
            else:
                progress = progress_placeholder.progress(0)
                table_container = results_placeholder.container()

                for i, review in enumerate(reviews, start=1):
                    pros, cons = analyze_one_review(review, model_name, max_items)
                    st.session_state.analysis_results.append({
                        "index": i,
                        "review": review,
                        "pros": pros,
                        "cons": cons
                    })

                    # å±•ç¤ºå½“å‰ç´¯è®¡ç»“æœï¼ˆè¡¨æ ¼å½¢å¼ï¼‰
                    display_rows = []
                    for row in st.session_state.analysis_results:
                        display_rows.append({
                            "åºå·": row["index"],
                            "è¯„ä»·": row["review"],
                            "ä¼˜ç‚¹": "ï¼›".join(row["pros"]) if row["pros"] else "",
                            "ç¼ºç‚¹": "ï¼›".join(row["cons"]) if row["cons"] else "",
                        })
                    table_container.dataframe(pd.DataFrame(display_rows), use_container_width=True, height=450)

                    progress.progress(i / total)
                    status_placeholder.info(f"å·²åˆ†æ {i}/{total} æ¡")

                status_placeholder.success("åˆ†æå®Œæˆã€‚å¯ç‚¹å‡»â€œç”Ÿæˆæ±‡æ€»ï¼ˆæµå¼è¾“å‡ºï¼‰â€")

if summary_clicked:
    if not api_key:
        st.error("è¯·åœ¨å·¦ä¾§è¾“å…¥ Google Gemini API Keyï¼ˆæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_API_KEYï¼‰")
    elif len(st.session_state.analysis_results) == 0:
        st.warning("è¯·å…ˆå®Œæˆé€æ¡åˆ†æã€‚")
    else:
        try:
            init_gemini(api_key)
        except Exception as e:
            st.error(f"åˆå§‹åŒ– Gemini å¤±è´¥ï¼š{e}")
        else:
            # æ±‡æ€»å»é‡
            all_pros: List[str] = []
            all_cons: List[str] = []
            for row in st.session_state.analysis_results:
                all_pros.extend(row.get("pros", []))
                all_cons.extend(row.get("cons", []))

            # å»é‡å¹¶ä¿åºï¼Œé¿å…æç¤ºè¿‡é•¿
            all_pros = unique_preserve(all_pros, limit=1000)
            all_cons = unique_preserve(all_cons, limit=1000)

            st.subheader("ğŸ§¾ æ±‡æ€»ï¼ˆæµå¼è¾“å‡ºï¼‰")
            with st.container(border=True):
                # æµå¼è¾“å‡º
                st.write_stream(stream_summary(all_pros, all_cons, model_name))
            st.session_state.summary_done = True


# -----------------------
# åº•éƒ¨è¯´æ˜
# -----------------------
with st.expander("ä½¿ç”¨è¯´æ˜"):
    st.markdown(
        "- ä¸Šä¼  CSV/Excelï¼Œé€‰æ‹©è¯„ä»·æ‰€åœ¨åˆ—ï¼›\n"
        "- ç‚¹å‡»â€œå¼€å§‹åˆ†æâ€é€æ¡æç‚¼ä¼˜ç¼ºç‚¹ï¼ˆä¼šå®æ—¶æ˜¾ç¤ºæ¯æ¡çš„åˆ†æç»“æœï¼‰ï¼›\n"
        "- å®Œæˆåç‚¹å‡»â€œç”Ÿæˆæ±‡æ€»ï¼ˆæµå¼è¾“å‡ºï¼‰â€ï¼Œå¾—åˆ°èšåˆåçš„äº§å“ä¼˜ç¼ºç‚¹ã€ç»“è®ºä¸å»ºè®®ï¼›\n"
        "- å¦‚éœ€æ›´å¿«æˆ–æ›´å¼ºåˆ†æï¼Œå¯åœ¨ä¾§è¾¹æ åˆ‡æ¢æ¨¡å‹ã€‚"
    )